{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dimensionality Reduction Techniques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Principle Component Analysis (PCA)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given a normalized dataset $\\mathbf{X} \\in R^{n\\times m}$ with $m$ variables and $n$ samples, PCA finds the set of vectors necessary to maximize the variance of the data along each principal component. To perform PCA, eigenvalue decomposition is done on the covariance matrix $\\mathbf{S}$ as shown the following equation.\r\n",
    "$$\r\n",
    "    \\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^T\\mathbf{X}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^T\r\n",
    "$$\r\n",
    "The diagonal matrix $\\boldsymbol{\\Lambda} \\in R^{m\\times m}$ contains the non-negative eigenvalues in decreasing order. The principal component matrix $\\mathbf{P}$ is formed by choosing $a$ eigenvectors in $\\mathbf{V}$ associated with the $a$ largest eigenvalues. The $a$ largest eigenvalues are stored in the matrix $\\boldsymbol{\\Lambda}_a$. Thus, $\\mathbf{P}$ and $\\boldsymbol{\\Lambda}_a$ can be used to transform $\\mathbf{X}$ into a lower dimensionality.\r\n",
    "\r\n",
    "Hotelling's $T^2$ statistic, Squared Prediction Error, and the novel covariance statistic are used to monitor faults. Thresholds are estimated using Kernel Density Estimation (KDE)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dynamic Inner PCA (DiPCA)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "While PCA aims to maximize the variance in each extracted principal component, PCA only extracts static cross-correlations. DiPCA accounts for the dynamics in the data by extracting dynamic latent variables which maximize auto-covariance (\\cite{dong2018novel}). The residual will have little to no auto-covariance and can be monitored using traditional PCA. \r\n",
    "\\\\Denote $\\mathbf{X} =\\begin{bmatrix}\\mathbf{x}_1&\\mathbf{x}_2&\\cdots&\\mathbf{x}_{n+s}\\end{bmatrix}^T$, where $s$ is the dynamic order of the model. Using $\\mathbf{X}$ the following equations form $\\mathbf{X_i}$ and $\\mathbf{Z_s}$.\r\n",
    "$$\r\n",
    "    \\mathbf{X_i} = \\begin{bmatrix}\\mathbf{x}_i&\\mathbf{x}_i+1&\\cdots&\\mathbf{x}_{n+i-1}\\end{bmatrix}^T\\,\\mathrm{for}\\, i=1,2,\\cdots ,s+1\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{Z_s} = \\begin{bmatrix}\\mathbf{X}_1&\\mathbf{X}_1&\\cdots&\\mathbf{X}_{s}\\end{bmatrix}\r\n",
    "$$\r\n",
    "Extracting the latent variables that maximize the auto-covariance can be formulated as\r\n",
    "$$\r\n",
    "    \\mathbf{w}, \\boldsymbol{\\beta} = \\argmax_{\\mathbf{w},\\boldsymbol{\\beta}}\\quad  \\mathbf{w}^T\\mathbf{X}_{s+1}^T\\mathbf{Z}_s\\left(\\boldsymbol{\\beta}\t\\otimes\\mathbf{w}\\right)\r\n",
    "$$\r\n",
    "$$\r\n",
    "    s.t.\\ \\ \\ ||\\mathbf{w}||=1,\\, ||\\boldsymbol{\\beta}||=1\r\n",
    "$$\r\n",
    "These equations can be solved using the algorithm given in Dong and Qin (2018). After obtainingw, the latent scorecomponent vectortand loading vectorpcan be calculated with\r\n",
    "$$\r\n",
    "    \\mathbf{t}=\\mathbf{X}\\mathbf{w}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{p}=\\mathbf{X}^T\\mathbf{t}/\\mathbf{t}^T\\mathbf{t}\r\n",
    "$$\r\n",
    "Xcan then be deflated with\r\n",
    "$$\r\n",
    "    \\mathbf{X}:=\\mathbf{X}-\\mathbf{t}\\mathbf{p}^T\r\n",
    "$$\r\n",
    "and deflatingX l times, l latent score component vectors and loading vectorsare  extracted.  Let $\\mathbf{T} = \\begin{bmatrix}\\mathbf{t_1}&\\mathbf{t_2}&\\cdots &\\mathbf{t_l}\\end{bmatrix}^T$ where $\\mathbf{t}_j,\\, j=1,2,\\cdots,l$ are the $jth$ latent component score vectors. $\\mathbf{T_i}$ can then be formed from $\\mathbf{T}$ similarly how $\\mathbf{X_i}$ is formed from $\\mathbf{X}$. An estimate for the vector autoregressive (VAR) model can be formulated using the following.\r\n",
    "$$\r\n",
    "    \\mathbf{\\bar{T}}_s=\\begin{bmatrix}\\mathbf{T_1}&\\mathbf{T_2}&\\cdots&\\mathbf{T_s}\\end{bmatrix}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\hat{\\boldsymbol{\\Theta}}=\\left(\\mathbf{\\bar{T}}_s^T\\mathbf{\\bar{T}}_s\\right)^{-1}\\mathbf{\\bar{T}}_s^T\\mathbf{T}_{s+1}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\hat{\\mathbf{T}}_{s+1}=\\mathbf{\\bar{T}}_s\\hat{\\boldsymbol{\\Theta}}\r\n",
    "$$\r\n",
    "\r\n",
    "Let $\\mathbf{W} = \\begin{bmatrix}\\mathbf{w_1}&\\mathbf{w_2}&\\cdots&\\mathbf{w_l}\\end{bmatrix}$ and $\\mathbf{P} = \\begin{bmatrix}\\mathbf{p_1}&\\mathbf{p_2}&\\cdots&\\mathbf{p_l}\\end{bmatrix}$. With $\\mathbf{W}$, $\\mathbf{P}$, and the VAR model, dynamic residuals $\\mathbf{v_k}$ and prediction errors $\\mathbf{e_k}$ can be calculated using\r\n",
    "\r\n",
    "$$\r\n",
    "    \\mathbf{R} = \\mathbf{W}\\left(\\mathbf{P}\\mathbf{W}\\right)^{-1}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{t_k} = \\mathbf{R}^T\\mathbf{x}_k\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{v_k} = \\mathbf{t_k}-\\mathbf{\\hat{t}}_k;\\,\\mathbf{\\hat{t}}_k = \\sum_{i=1}^s\\mathbf{\\hat{\\Theta}}_i^T\\mathbf{t}_{k-i}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{e_k} = \\mathbf{x_k}-\\mathbf{P}\\mathbf{\\hat{t_k}}\r\n",
    "$$\r\n",
    "$\\mathbf{\\hat{\\Theta}}_i$ is formed from $\\mathbf{\\hat{\\Theta}}$ similarly how $\\mathbf{X_i}$ is formed from $\\mathbf{X}$.  \\par Standard PCA is used to monitor $\\mathbf{v}_k$ and $\\mathbf{e}_k$. The combined index defined in Yue and Qin (2001) is used to monitor $\\mathbf{v}_k$ and is shown in\r\n",
    "\r\n",
    "$$\r\n",
    "    \\phi_v=\\mathbf{v}_k^T\\boldsymbol{\\Phi}_v\\mathbf{v}_k\r\n",
    "$$\r\n",
    "$$\r\n",
    "   \\boldsymbol{\\Phi}_v = \\frac{\\mathbf{P}_v\\boldsymbol{\\lambda}_v^{-1}\\mathbf{P}_v^T}{\\chi_v^2}+\\frac{\\mathbf{I}-\\mathbf{P}_v\\mathbf{P}_v^T}{\\delta_v^2}\r\n",
    "$$\r\n",
    "$\\mathbf{P}_v$ and $\\boldsymbol{\\lambda}_v$ are retained loading vectors and retained eigenvalues, respectively; $\\chi_v^2$ and $\\delta_v^2$ are the control limits for $T^2$ and $Q$, respectively. $\\mathbf{e}_k$ is monitored using the $T_r^2$ and $Q_r$ statistics. Control limits are determined using KDE.\r\n",
    "\r\n",
    "$\\mathbf{e}_k$ is monitored using $T_r^2$ and $Q_r$ as defined in the following equations.\r\n",
    "\r\n",
    "$$\r\n",
    "     T_r^2 = \\mathbf{e}_k^T\\mathbf{P}_r\\boldsymbol{\\lambda}_r^{-1}\\mathbf{P}_r^T\\mathbf{e}_k\r\n",
    "$$\r\n",
    "$$\r\n",
    "     Q_r = \\mathbf{e}_k^T\\left(\\mathbf{I}-\\mathbf{P}_r\\mathbf{P}_r^T\\right)\\mathbf{e}_k\r\n",
    "$$\r\n",
    "$\\boldsymbol{\\lambda}_r$ are retained loading vectors and retained eigenvalues, respectively. Control limits for $\\phi_v$, $T_r^2$, and $Q_r$ are determined using KDE."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Independent Component Analysis (ICA)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ICA aims to extract hidden independent components from mixed, non-gaussian data. Given a data-set with <i>n</i> samples and <i>m</i> zero-mean process variables, ICA assumes that the process variables $\\mathbf{X}$ = $[\\mathbf{x}_1, \\mathbf{x}_2,\\  \\ldots\\  \\mathbf{x}_\\mathit{m}]^T \\in R^{\\mathit{m}\\times 1}$ can be expressed as a linear combination of M independent source signals $\\mathbf{S} = [\\mathbf{s}_1, \\mathbf{s}_2,\\  \\ldots\\  \\mathbf{s}_\\mathit{m}]^T \\in R^{\\mathit{m}\\times 1}$.The relationship between the ICs and the original variables is given by:\r\n",
    "$$\r\n",
    "\\mathbf{x} = \\mathbf{As}\r\n",
    "$$\r\n",
    " <b>A</b> $\\in \\mathbb{R}^{m\\times m}$ is the unknown mixing matrix.\r\n",
    " The objective of FastICA can be defined as finding a demixing matrix $\\mathbf{W}\\in R^{m\\times m}$ such that the elements of the reconstructed source vector. \r\n",
    "$$\r\n",
    " \\mathbf{\\hat{s}} = \\mathbf{Wx}\r\n",
    "$$\r\n",
    "are as independent as possible. The eigen-decomposition of the covariance matrix $\\textbf{C}_x=E(\\textbf{xx}^T)\\in R^{M\\times M}$ can be given by\r\n",
    "$$\r\n",
    "     \\mathbf{C}_x=\\mathbf{V}\\boldsymbol{\\Lambda} \\mathbf{V}^T\r\n",
    "$$\r\n",
    "where $\\boldsymbol{\\Lambda} \\in R^{m\\times m}$ is a diagonal matrix of the eigenvalues of $\\mathbf{C}_x$ and $\\mathbf{V}$.\r\n",
    "The whitening transformation is implemented as:\r\n",
    "$$\r\n",
    "     \\mathbf{z}=\\mathbf{Qx}=\\boldsymbol{QAs} = \\mathbf{Us}\r\n",
    "$$\r\n",
    "\r\n",
    "<b>z</b> holds the whitened variables, $\\mathbf{Q} = \\mathbf{\\Lambda} ^{-1/2} \\mathbf{V}^T$ is the whitening matrix, $\\mathbf{U} = \\mathbf{QA} = [\\mathbf{u}_1, \\mathbf{u}_2,\\  \\ldots\\  \\mathbf{u}_\\mathit{m}]$ is the orthogonal extracting matrix.\r\n",
    "The relationship between $\\mathbf{W}$ and $\\mathbf{Q}$ is:\r\n",
    "$$\r\n",
    "    \\mathbf{W}=\\mathbf{U}^T\\boldsymbol{Q}\r\n",
    "$$\r\n",
    "\r\n",
    "to calculate $\\textbf{U}$ FastICA takes the following negentropy approximation as its objective function.\r\n",
    "$$\r\n",
    "      \\max_\\mathbf{u}\\ \\ \\ \\{E[G(\\mathbf{u}^T\\mathbf{z})^2]-E[G(v)]\\}^2 \r\n",
    "$$\r\n",
    "$$\r\n",
    "s.t.\\ \\ \\ E[(\\mathbf{u}^T\\mathbf{z})^2]=1\r\n",
    "$$\r\n",
    "\r\n",
    "$\\mathbf{u}$ is a column of $\\mathbf{U}$, $v$ is the Gaussian variable with zero mean and unit variance, $G(x)$ is a non quadratic function which is chosen to be $G(x)=-exp(-x^2 /2)$.\r\n",
    "Next, the IC  can be computed using equations demixing and whitening/demixing equations shown earlier.\r\n",
    "For fault detection, the three monitoring statistics can be defined as\r\n",
    "$$\r\n",
    "     I^2 = (\\mathbf{U}_p^T\\mathbf{z})^T(\\mathbf{U}_p^T\\mathbf{z})\r\n",
    "$$\r\n",
    "$$\r\n",
    "    I_e^2 = (\\mathbf{U}_{\\tilde{p}}^T\\mathbf{z})^T(\\mathbf{U}_{\\tilde{p}}^T\\mathbf{z})\r\n",
    "$$\r\n",
    "$$\r\n",
    "     SPE = (\\mathbf{x}-\\mathbf{Q}^{-1}\\mathbf{U}_p\\mathbf{U}_p^T\\mathbf{Qx})^T(\\mathbf{x}-\\mathbf{Q}^{-1}\\mathbf{U}_p\\mathbf{U}_p^T\\mathbf{Qx})\r\n",
    "$$\r\n",
    "where $\\textbf{U}_p$ is composed of the first $\\textit{p}$ columns of $\\textbf{U}$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Canonical Variate Analysis (CVA)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CVA for fault detection aims to maximize the correlation between past and future data in a given set.\r\n",
    "For a data-set $\\mathbf{X} \\in R^{n \\times m}$ centered about zero, with $\\textit{m}$ variables and $\\textit{n}$ samples, a past and future vector is constructed for each considered time step. Each vector contains the data points q time steps before/after the point being considered. let $\\mathbf{x}_k$ represent a sample(row) taken from $\\textbf{X}$ at timestep $\\textit{k}$. the past and future vectors are stacked lengthwise.\r\n",
    "\r\n",
    "$$\r\n",
    "    \\mathbf{p}_k = [\\mathbf{x}_{k}\\ \\  \\mathbf{x}_{k-1}\\ \\  \\mathbf{x}_{k-2}\\ \\ \\ldots\\ \\  \\mathbf{x}_{k-q+1}]^T \\in R^{mq}\r\n",
    "$$\r\n",
    "$$\r\n",
    "%     \\mathbf{f}_k = [\\mathbf{x}_{k+1}\\ \\  \\mathbf{x}_{k+2}\\ \\  \\mathbf{x}_{k+3}\\ \\  \\ldots\\ \\  \\mathbf{x}_{k+q}]^T \\in R^{mq}\r\n",
    "$$\r\n",
    "where q is the number of timesteps into the past and future the past and future windows extend, m is the number of variables, and k is the timestep being considered starting at q timesteps from the first observation and ending q steps before the last observation to accommodate for the length of the window. This range can be represented as all $k \\in [q, n-2q]$. \\par\r\n",
    "Both vectors are normalized to zero mean and unit variance.\r\n",
    "Then the $\\mathbf{p}_k$ and $\\mathbf{f}_k$ vectors for all $k \\in [q, n-q]$ are appended column-wise to form past and future Hankel matrices.\r\n",
    "\r\n",
    "$$\r\n",
    "    \\mathbf{Y}_p=[\\mathbf{p}_{q+1}\\ \\ \\mathbf{p}_{q+2}\\ \\ \\ldots \\ \\ \\mathbf{p}_{n-q}]\\in R^{mq\\times N-2q+1}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{Y}_f=[\\mathbf{f}_{q+1}\\ \\ \\mathbf{f}_{q+2}\\ \\ \\ldots \\ \\ \\mathbf{f}_{n-q}]\\in R^{mq\\times N-2q+1}\r\n",
    "$$\r\n",
    "\r\n",
    "The covariance and cross-covariance of the past and future matrices can be estimated as:\r\n",
    "$$\r\n",
    "    \\boldsymbol{\\Sigma}_{pp}=\\frac{1}{n-2q}\\mathbf{Y}_p\\mathbf{Y}_p^T \\in R^{mq\\times mq}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\boldsymbol{\\Sigma}_{ff}=\\frac{1}{n-2q}\\mathbf{Y}_f\\mathbf{Y}_f^T \\in R^{mq\\times mq}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\boldsymbol{\\Sigma}_{fp}=\\frac{1}{n-2q}\\mathbf{Y}_f\\mathbf{Y}_p^T \\in R^{mq\\times mq}\r\n",
    "$$\r\n",
    "\r\n",
    "Next, to find the linear combination with the maximum correlation, Singular Value Decomposition(SVD) is performed on the Hankel matrix \\textbf{H}.\r\n",
    "$$\r\n",
    "    \\mathbf{H}=\\boldsymbol{\\Sigma}_{ff}^{-1/2}\\boldsymbol{\\Sigma}_{fp}\\boldsymbol{\\Sigma}_{pp}^{-1/2}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^T\r\n",
    "$$\r\n",
    "\r\n",
    "$\\textbf{U}$ and $\\textbf{V}$ are the left and right singular column vectors of $\\textbf{H}$ and $\\boldsymbol{\\Sigma}=diag(\\sigma_1, \\sigma_1, \\sigma_1 \\ldots \\sigma_1, 0 \\ldots 0)$ of singular values ordered from largest to smallest, where $\\textit{r}$ is the rank of $\\textbf{H}$\r\n",
    "\r\n",
    "\r\n",
    "To get the $T^2$ and SPE first the state vector, $z_k$, and the residual vector, $e_k$, must be calculated for each timestep:\r\n",
    "$$\r\n",
    "    \\mathbf{z}_k=\\mathbf{J}_l\\mathbf{p}_k \\in R^{l}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{e}_k=\\mathbf{F}\\mathbf{p}_k \\in R^{l}\r\n",
    "$$\r\n",
    "$\\mathbf{J}_l=\\mathbf{V}_l^T\\boldsymbol{\\Sigma}_{pp}^{-1/2}$, $\\mathbf{F}=(\\mathbf{I}-\\mathbf{V}_l\\mathbf{V}_l^T)\\boldsymbol{\\Sigma}_{pp}^{-1/2}$ and $\\mathbf{V}_l$ is a reduced matrix consisting of the first \\textit{l} columns of \\textbf{V}. Matrices $\\mathbf{J}_l$ and $\\mathbf{F}$ represent the projection matrices applied to the past data vectors.\r\n",
    "\r\n",
    "Next, $T^2$ and $\\textit{Q}$ statistics are calculated.\r\n",
    "$$\r\n",
    "    T^2_k = \\mathbf{z}_k^T\\mathbf{z}_k\r\n",
    "$$\r\n",
    "$$\r\n",
    "    Q_k = \\mathbf{e}_k^T\\mathbf{e}_k\r\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agglomerative"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linkage Types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Covariance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Euclidean Combinations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Distributions and Statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hotelling's $\\mathbf{T^2}$ Distribution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Squared Prediction Error (SPE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Covariance Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel Density Estimation (KDE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}