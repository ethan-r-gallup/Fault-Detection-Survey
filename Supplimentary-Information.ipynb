{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a normalized dataset $\\mathbf{X} \\in R^{n\\times m}$ with $m$ variables and $n$ samples, PCA finds the set of vectors necessary to maximize the variance of the data along each principal component. To perform PCA, eigenvalue decomposition is done on the covariance matrix $\\mathbf{S}$ as shown the following equation.\r\n",
    "$$\r\n",
    "    \\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^T\\mathbf{X}=\\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^T\r\n",
    "$$\r\n",
    "The diagonal matrix $\\boldsymbol{\\Lambda} \\in R^{m\\times m}$ contains the non-negative eigenvalues in decreasing order. The principal component matrix $\\mathbf{P}$ is formed by choosing $a$ eigenvectors in $\\mathbf{V}$ associated with the $a$ largest eigenvalues. The $a$ largest eigenvalues are stored in the matrix $\\boldsymbol{\\Lambda}_a$. Thus, $\\mathbf{P}$ and $\\boldsymbol{\\Lambda}_a$ can be used to transform $\\mathbf{X}$ into a lower dimensionality.\r\n",
    "\r\n",
    "Hotelling's $T^2$ statistic, Squared Prediction Error, and the novel covariance statistic are used to monitor faults. Thresholds are estimated using Kernel Density Estimation (KDE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Inner PCA (DiPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PCA aims to maximize the variance in each extracted principal component, PCA only extracts static cross-correlations. DiPCA accounts for the dynamics in the data by extracting dynamic latent variables which maximize auto-covariance (\\cite{dong2018novel}). The residual will have little to no auto-covariance and can be monitored using traditional PCA. \r\n",
    "\\\\Denote $\\mathbf{X} =\\begin{bmatrix}\\mathbf{x}_1&\\mathbf{x}_2&\\cdots&\\mathbf{x}_{n+s}\\end{bmatrix}^T$, where $s$ is the dynamic order of the model. Using $\\mathbf{X}$ the following equations form $\\mathbf{X_i}$ and $\\mathbf{Z_s}$.\r\n",
    "$$\r\n",
    "    \\mathbf{X_i} = \\begin{bmatrix}\\mathbf{x}_i&\\mathbf{x}_i+1&\\cdots&\\mathbf{x}_{n+i-1}\\end{bmatrix}^T\\,\\mathrm{for}\\, i=1,2,\\cdots ,s+1\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{Z_s} = \\begin{bmatrix}\\mathbf{X}_1&\\mathbf{X}_1&\\cdots&\\mathbf{X}_{s}\\end{bmatrix}\r\n",
    "$$\r\n",
    "Extracting the latent variables that maximize the auto-covariance can be formulated as\r\n",
    "$$\r\n",
    "    \\mathbf{w}, \\boldsymbol{\\beta} = \\argmax_{\\mathbf{w},\\boldsymbol{\\beta}}\\quad  \\mathbf{w}^T\\mathbf{X}_{s+1}^T\\mathbf{Z}_s\\left(\\boldsymbol{\\beta}\t\\otimes\\mathbf{w}\\right)\r\n",
    "$$\r\n",
    "$$\r\n",
    "    s.t.\\ \\ \\ ||\\mathbf{w}||=1,\\, ||\\boldsymbol{\\beta}||=1\r\n",
    "$$\r\n",
    "These equations can be solved using the algorithm given in Dong and Qin (2018). After obtainingw, the latent scorecomponent vectortand loading vectorpcan be calculated with\r\n",
    "$$\r\n",
    "    \\mathbf{t}=\\mathbf{X}\\mathbf{w}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{p}=\\mathbf{X}^T\\mathbf{t}/\\mathbf{t}^T\\mathbf{t}\r\n",
    "$$\r\n",
    "Xcan then be deflated with\r\n",
    "$$\r\n",
    "    \\mathbf{X}:=\\mathbf{X}-\\mathbf{t}\\mathbf{p}^T\r\n",
    "$$\r\n",
    "and deflatingX l times, l latent score component vectors and loading vectorsare  extracted.  Let $\\mathbf{T} = \\begin{bmatrix}\\mathbf{t_1}&\\mathbf{t_2}&\\cdots &\\mathbf{t_l}\\end{bmatrix}^T$ where $\\mathbf{t}_j,\\, j=1,2,\\cdots,l$ are the $jth$ latent component score vectors. $\\mathbf{T_i}$ can then be formed from $\\mathbf{T}$ similarly how $\\mathbf{X_i}$ is formed from $\\mathbf{X}$. An estimate for the vector autoregressive (VAR) model can be formulated using the following.\r\n",
    "$$\r\n",
    "    \\mathbf{\\bar{T}}_s=\\begin{bmatrix}\\mathbf{T_1}&\\mathbf{T_2}&\\cdots&\\mathbf{T_s}\\end{bmatrix}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\hat{\\boldsymbol{\\Theta}}=\\left(\\mathbf{\\bar{T}}_s^T\\mathbf{\\bar{T}}_s\\right)^{-1}\\mathbf{\\bar{T}}_s^T\\mathbf{T}_{s+1}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\hat{\\mathbf{T}}_{s+1}=\\mathbf{\\bar{T}}_s\\hat{\\boldsymbol{\\Theta}}\r\n",
    "$$\r\n",
    "\r\n",
    "Let $\\mathbf{W} = \\begin{bmatrix}\\mathbf{w_1}&\\mathbf{w_2}&\\cdots&\\mathbf{w_l}\\end{bmatrix}$ and $\\mathbf{P} = \\begin{bmatrix}\\mathbf{p_1}&\\mathbf{p_2}&\\cdots&\\mathbf{p_l}\\end{bmatrix}$. With $\\mathbf{W}$, $\\mathbf{P}$, and the VAR model, dynamic residuals $\\mathbf{v_k}$ and prediction errors $\\mathbf{e_k}$ can be calculated using\r\n",
    "\r\n",
    "$$\r\n",
    "    \\label{eq:dipca_weighted_loadings}\r\n",
    "    \\mathbf{R} = \\mathbf{W}\\left(\\mathbf{P}\\mathbf{W}\\right)^{-1}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{t_k} = \\mathbf{R}^T\\mathbf{x}_k\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{v_k} = \\mathbf{t_k}-\\mathbf{\\hat{t}}_k;\\,\\mathbf{\\hat{t}}_k = \\sum_{i=1}^s\\mathbf{\\hat{\\Theta}}_i^T\\mathbf{t}_{k-i}\r\n",
    "$$\r\n",
    "$$\r\n",
    "    \\mathbf{e_k} = \\mathbf{x_k}-\\mathbf{P}\\mathbf{\\hat{t_k}}\r\n",
    "$$\r\n",
    "$\\mathbf{\\hat{\\Theta}}_i$ is formed from $\\mathbf{\\hat{\\Theta}}$ similarly how $\\mathbf{X_i}$ is formed from $\\mathbf{X}$.  \\par Standard PCA is used to monitor $\\mathbf{v}_k$ and $\\mathbf{e}_k$. The combined index defined in Yue and Qin (2001) is used to monitor $\\mathbf{v}_k$ and is shown in\r\n",
    "\r\n",
    "$$\r\n",
    "    \\phi_v=\\mathbf{v}_k^T\\boldsymbol{\\Phi}_v\\mathbf{v}_k\r\n",
    "$$\r\n",
    "$$\r\n",
    "   \\boldsymbol{\\Phi}_v = \\frac{\\mathbf{P}_v\\boldsymbol{\\lambda}_v^{-1}\\mathbf{P}_v^T}{\\chi_v^2}+\\frac{\\mathbf{I}-\\mathbf{P}_v\\mathbf{P}_v^T}{\\delta_v^2}\r\n",
    "$$\r\n",
    "$\\mathbf{P}_v$ and $\\boldsymbol{\\lambda}_v$ are retained loading vectors and retained eigenvalues, respectively; $\\chi_v^2$ and $\\delta_v^2$ are the control limits for $T^2$ and $Q$, respectively. $\\mathbf{e}_k$ is monitored using the $T_r^2$ and $Q_r$ statistics. Control limits are determined using KDE.\r\n",
    "\r\n",
    "$\\mathbf{e}_k$ is monitored using $T_r^2$ and $Q_r$ as defined in the following equations.\r\n",
    "\r\n",
    "$$\r\n",
    "     T_r^2 = \\mathbf{e}_k^T\\mathbf{P}_r\\boldsymbol{\\lambda}_r^{-1}\\mathbf{P}_r^T\\mathbf{e}_k\r\n",
    "$$\r\n",
    "$$\r\n",
    "     Q_r = \\mathbf{e}_k^T\\left(\\mathbf{I}-\\mathbf{P}_r\\mathbf{P}_r^T\\right)\\mathbf{e}_k\r\n",
    "$$\r\n",
    "\\boldsymbol{\\lambda}_r$ are retained loading vectors and retained eigenvalues, respectively. Control limits for $\\phi_v$, $T_r^2$, and $Q_r$ are determined using KDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Component Analysis (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICA aims to extract hidden independent components from mixed, non-gaussian data. Given a data-set with <i>n</i> samples and <i>m</i> zero-mean process variables, ICA assumes that the process variables $\\mathbf{X}$ = $[\\mathbf{x}_1, \\mathbf{x}_2,\\  \\ldots\\  \\mathbf{x}_\\mathit{m}]^T \\in R^{\\mathit{m}\\times 1}$ can be expressed as a linear combination of M independent source signals $\\mathbf{S} = [\\mathbf{s}_1, \\mathbf{s}_2,\\  \\ldots\\  \\mathbf{s}_\\mathit{m}]^T \\in R^{\\mathit{m}\\times 1}$.The relationship between the ICs and the original variables is given by:\r\n",
    "$$\r\n",
    "\\mathbf{x} = \\mathbf{As}\r\n",
    "$$\r\n",
    " <b>A</b> $\\in \\mathbb{R}^{m\\times m}$ is the unknown mixing matrix.\r\n",
    " The objective of FastICA can be defined as finding a demixing matrix $\\mathbf{W}\\in R^{m\\times m}$ such that the elements of the reconstructed source vector. \r\n",
    "$$\r\n",
    " \\mathbf{\\hat{s}} = \\mathbf{Wx}\r\n",
    "$$\r\n",
    "are as independent as possible. The eigen-decomposition of the covariance matrix $\\textbf{C}_x=E(\\textbf{xx}^T)\\in R^{M\\times M}$ can be given by\r\n",
    "$$\r\n",
    "     \\mathbf{C}_x=\\mathbf{V}\\boldsymbol{\\Lambda} \\mathbf{V}^T\r\n",
    "$$\r\n",
    "where $\\boldsymbol{\\Lambda} \\in R^{m\\times m}$ is a diagonal matrix of the eigenvalues of $\\mathbf{C}_x$ and $\\mathbf{V}$.\r\n",
    "The whitening transformation is implemented as:\r\n",
    "$$\r\n",
    "     \\mathbf{z}=\\mathbf{Qx}=\\boldsymbol{QAs} = \\mathbf{Us}\r\n",
    "$$\r\n",
    "\r\n",
    "<b>z</b> holds the whitened variables, $\\mathbf{Q} = \\mathbf{\\Lambda} ^{-1/2} \\mathbf{V}^T$ is the whitening matrix, $\\mathbf{U} = \\mathbf{QA} = [\\mathbf{u}_1, \\mathbf{u}_2,\\  \\ldots\\  \\mathbf{u}_\\mathit{m}]$ is the orthogonal extracting matrix.\r\n",
    "The relationship between $\\mathbf{W}$ and $\\mathbf{Q}$ is:\r\n",
    "$$\r\n",
    "    \\mathbf{W}=\\mathbf{U}^T\\boldsymbol{Q}\r\n",
    "$$\r\n",
    "\r\n",
    "to calculate $\\textbf{U}$ FastICA takes the following negentropy approximation as its objective function.\r\n",
    "$$\r\n",
    "      \\max_\\mathbf{u}\\ \\ \\ \\{E[G(\\mathbf{u}^T\\mathbf{z})^2]-E[G(v)]\\}^2 \r\n",
    "$$\r\n",
    "$$\r\n",
    "s.t.\\ \\ \\ E[(\\mathbf{u}^T\\mathbf{z})^2]=1\r\n",
    "$$\r\n",
    "\r\n",
    "$\\mathbf{u}$ is a column of $\\mathbf{U}$, $v$ is the Gaussian variable with zero mean and unit variance, $G(x)$ is a non quadratic function which is chosen to be $G(x)=-exp(-x^2 /2)$.\r\n",
    "Next, the IC  can be computed using equations demixing and whitening/demixing equations shown earlier.\r\n",
    "For fault detection, the three monitoring statistics can be defined as\r\n",
    "$$\r\n",
    "     I^2 = (\\mathbf{U}_p^T\\mathbf{z})^T(\\mathbf{U}_p^T\\mathbf{z})\r\n",
    "$$\r\n",
    "$$\r\n",
    "    I_e^2 = (\\mathbf{U}_{\\tilde{p}}^T\\mathbf{z})^T(\\mathbf{U}_{\\tilde{p}}^T\\mathbf{z})\r\n",
    "$$\r\n",
    "$$\r\n",
    "     SPE = (\\mathbf{x}-\\mathbf{Q}^{-1}\\mathbf{U}_p\\mathbf{U}_p^T\\mathbf{Qx})^T(\\mathbf{x}-\\mathbf{Q}^{-1}\\mathbf{U}_p\\mathbf{U}_p^T\\mathbf{Qx})\r\n",
    "$$\r\n",
    "where $\\textbf{U}_p$ is composed of the first $\\textit{p}$ columns of $\\textbf{U}$."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}