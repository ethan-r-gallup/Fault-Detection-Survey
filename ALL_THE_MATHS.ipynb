{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a normalized dataset $\\mathbf{X} \\in R^{n\\times m}$ with $m$ variables and $n$ samples, PCA finds the set of vectors necessary to maximize the variance of the data along each principal component. PCA can be looked at as an optimization problem in which a series of perpendicular vectors(components) are drawn such that variance along the direction of each component and minimized perpendicular to(around) the principle components.\n",
    "\n",
    "<img src=\"images/var.png\" width=400 height=200 align=\"center\">\n",
    "\n",
    "This can be achieved by getting the eigendecomposition of the covariance matrix. The steps to do so are as follows.\n",
    "\n",
    "The first step, after the data is normalized, is to compute the covariance matrix.\n",
    "\n",
    "1. \n",
    "$$\n",
    "    \\boldsymbol{\\Sigma} = \\frac{1}{n-1}\\mathbf{X}^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "Next, to get the eigenvectors and eigenvalues, singular value decomposition (SVD) is performed on the covariance matrix.\n",
    "\n",
    "2. \n",
    "$$\n",
    "    \\boldsymbol{\\Sigma} = \\frac{1}{n-1}\\mathbf{X}^T\\mathbf{X}=\\mathbf{P}\\boldsymbol{\\Lambda}\\mathbf{P}^T\n",
    "$$\n",
    "\n",
    "where the diagonal matrix $\\boldsymbol{\\Lambda} \\in R^{m\\times m}$ contains the non-negative eigenvalues in decreasing order. Each eigenvalue represents the explained variance of the corresponding eigenvector in $\\mathbf{V}$.  The $a$ largest eigenvalues make up the principle eigenvalues $\\boldsymbol{\\Lambda}_{pc}=diag(\\lambda_1 \\dots \\lambda_a) \\in R^{a\\times a}$ and the remaining make up the residual eigenvalues $\\boldsymbol{\\Lambda}_{res}=diag(\\lambda_{a+1} \\dots \\lambda_{m}) \\in R^{(m-a) \\times (m-a)}$.  The principal component matrix $\\mathbf{P}$ is formed by choosing $a$ eigenvectors in $\\mathbf{V}$ the eigenvalues in $\\boldsymbol{\\Lambda}_{pc}$.\n",
    " \n",
    " 3. \n",
    "$$\n",
    "\\boldsymbol{\\Lambda} = \n",
    "\\begin{bmatrix} \n",
    "    \\lambda_1 & 0 & \\dots & 0 & \\dots& 0 \\\\\n",
    "    0 & \\lambda_2 & \\dots & 0 & \\dots& 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "    0 & 0 & \\dots & \\lambda _a & \\dots& 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "    0 & 0 & \\dots & 0 & \\dots & \\lambda _m \\\\\n",
    "\\end{bmatrix} , \\ \\ \\ \\ \n",
    "\\boldsymbol{\\Lambda}_{pc} = \n",
    "\\begin{bmatrix} \n",
    "    \\lambda_1 & 0 & \\dots & 0 \\\\\n",
    "    0 & \\lambda_2 & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\lambda _a \\\\\n",
    "\\end{bmatrix} , \\ \\ \\ \\ \n",
    "\\boldsymbol{\\Lambda}_{res} = \n",
    "\\begin{bmatrix} \n",
    "    \\lambda_{a+1} & 0 & \\dots & 0 \\\\\n",
    "    0 & \\lambda_{a+2} & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\lambda _m \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$a$ is determined by finding the number of eigenvalues(explained variance) add up to be some percentage of the total variance. In our case we are using 99%.\n",
    "\n",
    "4. \n",
    "$$\n",
    "\\large{\\frac{\\sum_{i=1}^{a}\\lambda_i}{\\sum_{i=1}^{m}\\lambda_i}} = 0.99\n",
    "$$\n",
    "\n",
    "Hotelling's $T^2$ statistic and upper control limit can be found using the following equations.\n",
    "\n",
    "5. \n",
    "$$\n",
    "t^2=\\mathbf{x}_i^T\\mathbf{P}\\boldsymbol{\\Lambda}^{-1}_a\\mathbf{P}^T\\mathbf{x}_i\n",
    "$$\n",
    "6. \n",
    "$$\n",
    "T^2_{\\alpha} = \\frac{a(n-1)}{n-a}F_{\\mathit{a},\\mathit{n-a},\\ \\  \\alpha}\n",
    "$$\n",
    "the squared prediction error and its upper control limit can be found using equation 7 through 9.\n",
    "\n",
    "7. \n",
    "$$\n",
    "Q = \\mathbf{x}_i^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "8. \n",
    "$$\n",
    "Q_{\\alpha} = \\theta_1 \\left(\\frac{h_0c_{\\alpha}\\sqrt{2\\theta_2}}{\\theta_1} + 1 + \\frac{\\theta_2 h_0 (h_0-1)}{\\theta_1^2}\\right)^{\\frac{1}{h_0}}\n",
    "$$ \n",
    "where\n",
    "\n",
    "9. \n",
    "$$\n",
    "\\theta_i = \\sum_{j=a+1}^m(\\lambda_j^2)^i\\ , \\ \\ \\ \\ \\ \\  i = 1, 2, 3\\ , \\ \\ \\ \\ \\ \\  h_0=1-\\frac{2\\theta_1 \\theta_3}{3\\theta_2^2}\n",
    "$$\n",
    "\n",
    "<b>More information on these statistics and their upper control limits can be found below in the <i>Distributions and Statistics</i> section.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Inner PCA (DiPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PCA aims to maximize the variance in each extracted principal component, PCA only extracts static cross-correlations. DiPCA accounts for the dynamics in the data by extracting dynamic latent variables which maximize auto-covariance (Dong and Qin (2018)). The residual will have little to no auto-covariance and can be monitored using traditional PCA. \n",
    "\\\\Denote $\\mathbf{X} =\\begin{bmatrix}\\mathbf{x}_1&\\mathbf{x}_2&\\cdots&\\mathbf{x}_{n+s}\\end{bmatrix}^T$, where $s$ is the dynamic order of the model. Using $\\mathbf{X}$ the following equations form $\\mathbf{X_i}$ and $\\mathbf{Z_s}$.\n",
    "\n",
    "10. \n",
    "$$\n",
    "    \\mathbf{X_i} = \\begin{bmatrix}\\mathbf{x}_i&\\mathbf{x}_i+1&\\cdots&\\mathbf{x}_{n+i-1}\\end{bmatrix}^T\\,\\mathrm{for}\\, i=1,2,\\cdots ,s+1\n",
    "$$\n",
    "11. \n",
    "$$\n",
    "    \\mathbf{Z_s} = \\begin{bmatrix}\\mathbf{X}_1&\\mathbf{X}_1&\\cdots&\\mathbf{X}_{s}\\end{bmatrix}\n",
    "$$\n",
    "Extracting the latent variables that maximize the auto-covariance can be formulated as\n",
    "\n",
    "12. \n",
    "$$\n",
    "    \\mathbf{w}, \\boldsymbol{\\beta} = \\argmax_{\\mathbf{w},\\boldsymbol{\\beta}}\\quad  \\mathbf{w}^T\\mathbf{X}_{s+1}^T\\mathbf{Z}_s\\left(\\boldsymbol{\\beta}\t\\otimes\\mathbf{w}\\right)\n",
    "$$\n",
    "13. \n",
    "$$\n",
    "    s.t.\\ \\ \\ ||\\mathbf{w}||=1,\\, ||\\boldsymbol{\\beta}||=1\n",
    "$$\n",
    "These equations can be solved using the algorithm given in (Dong and Qin (2018)). After obtaining w, the latent score component vectort and loading vector $p$ can be calculated with\n",
    "\n",
    "14. \n",
    "$$\n",
    "    \\mathbf{t}=\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "15. \n",
    "$$\n",
    "    \\mathbf{p}=\\mathbf{X}^T\\mathbf{t}/\\mathbf{t}^T\\mathbf{t}\n",
    "$$\n",
    "X can then be deflated with\n",
    "\n",
    "16. \n",
    "$$\n",
    "    \\mathbf{X}:=\\mathbf{X}-\\mathbf{t}\\mathbf{p}^T\n",
    "$$\n",
    "and deflating $\\mathbf{X}$ $l$ times, $l$ latent score component vectors and loading vectorsare  extracted.  Let $\\mathbf{T} = \\begin{bmatrix}\\mathbf{t_1}&\\mathbf{t_2}&\\cdots &\\mathbf{t_l}\\end{bmatrix}^T$ where $\\mathbf{t}_j,\\, j=1,2,\\cdots,l$ are the $jth$ latent component score vectors. $\\mathbf{T_i}$ can then be formed from $\\mathbf{T}$ similarly how $\\mathbf{X_i}$ is formed from $\\mathbf{X}$. An estimate for the vector autoregressive (VAR) model can be formulated using the following.\n",
    "\n",
    "17. \n",
    "$$\n",
    "    \\mathbf{\\bar{T}}_s=\\begin{bmatrix}\\mathbf{T_1}&\\mathbf{T_2}&\\cdots&\\mathbf{T_s}\\end{bmatrix}\n",
    "$$\n",
    "18. \n",
    "$$\n",
    "    \\hat{\\boldsymbol{\\Theta}}=\\left(\\mathbf{\\bar{T}}_s^T\\mathbf{\\bar{T}}_s\\right)^{-1}\\mathbf{\\bar{T}}_s^T\\mathbf{T}_{s+1}\n",
    "$$\n",
    "19. \n",
    "$$\n",
    "    \\hat{\\mathbf{T}}_{s+1}=\\mathbf{\\bar{T}}_s\\hat{\\boldsymbol{\\Theta}}\n",
    "$$\n",
    "\n",
    "Let $\\mathbf{W} = \\begin{bmatrix}\\mathbf{w_1}&\\mathbf{w_2}&\\cdots&\\mathbf{w_l}\\end{bmatrix}$ and $\\mathbf{P} = \\begin{bmatrix}\\mathbf{p_1}&\\mathbf{p_2}&\\cdots&\\mathbf{p_l}\\end{bmatrix}$. With $\\mathbf{W}$, $\\mathbf{P}$, and the VAR model, dynamic residuals $\\mathbf{v_k}$ and prediction errors $\\mathbf{e_k}$ can be calculated using\n",
    "\n",
    "20. \n",
    "$$\n",
    "    \\mathbf{R} = \\mathbf{W}\\left(\\mathbf{P}\\mathbf{W}\\right)^{-1}\n",
    "$$\n",
    "21. \n",
    "$$\n",
    "    \\mathbf{t_k} = \\mathbf{R}^T\\mathbf{x}_k\n",
    "$$\n",
    "22. \n",
    "$$\n",
    "    \\mathbf{v_k} = \\mathbf{t_k}-\\mathbf{\\hat{t}}_k;\\,\\mathbf{\\hat{t}}_k = \\sum_{i=1}^s\\mathbf{\\hat{\\Theta}}_i^T\\mathbf{t}_{k-i}\n",
    "$$\n",
    "23. \n",
    "$$\n",
    "    \\mathbf{e_k} = \\mathbf{x_k}-\\mathbf{P}\\mathbf{\\hat{t_k}}\n",
    "$$\n",
    "$\\mathbf{\\hat{\\Theta}}_i$ is formed from $\\mathbf{\\hat{\\Theta}}$ similarly how $\\mathbf{X_i}$ is formed from $\\mathbf{X}$.\n",
    "Standard PCA is used to monitor $\\mathbf{v}_k$ and $\\mathbf{e}_k$. The combined index defined in Yue and Qin (2001) is used to monitor $\\mathbf{v}_k$ and is shown in\n",
    "\n",
    "24. \n",
    "$$\n",
    "    \\phi_v=\\mathbf{v}_k^T\\boldsymbol{\\Phi}_v\\mathbf{v}_k\n",
    "$$\n",
    "25. \n",
    "$$\n",
    "   \\boldsymbol{\\Phi}_v = \\frac{\\mathbf{P}_v\\boldsymbol{\\lambda}_v^{-1}\\mathbf{P}_v^T}{\\chi_v^2}+\\frac{\\mathbf{I}-\\mathbf{P}_v\\mathbf{P}_v^T}{\\delta_v^2}\n",
    "$$\n",
    "$\\mathbf{P}_v$ and $\\boldsymbol{\\lambda}_v$ are retained loading vectors and retained eigenvalues, respectively; $\\chi_v^2$ and $\\delta_v^2$ are the control limits for $T^2$ and $Q$, respectively. $\\mathbf{e}_k$ is monitored using the $T_r^2$ and $Q_r$ statistics. Control limits are determined using KDE.\n",
    "\n",
    "$\\mathbf{e}_k$ is monitored using $T_r^2$ and $Q_r$ as defined in the following equations.\n",
    "\n",
    "26. \n",
    "$$\n",
    "     T_r^2 = \\mathbf{e}_k^T\\mathbf{P}_r\\boldsymbol{\\lambda}_r^{-1}\\mathbf{P}_r^T\\mathbf{e}_k\n",
    "$$\n",
    "27. \n",
    "$$\n",
    "     Q_r = \\mathbf{e}_k^T\\left(\\mathbf{I}-\\mathbf{P}_r\\mathbf{P}_r^T\\right)\\mathbf{e}_k\n",
    "$$\n",
    "$\\boldsymbol{\\lambda}_r$ are retained loading vectors and retained eigenvalues, respectively. \n",
    "\n",
    "<b>More information on these statistics and their upper control limits can be found below in the <i>Distributions and Statistics</i> section.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Component Analysis (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICA aims to extract hidden independent components from mixed, non-gaussian data. Given a data-set with <i>n</i> samples and <i>m</i> zero-mean process variables, ICA assumes that the process variables $\\mathbf{X}$ = $[\\mathbf{x}_1, \\mathbf{x}_2,\\  \\ldots\\  \\mathbf{x}_\\mathit{m}]^T \\in R^{\\mathit{m}\\times 1}$ can be expressed as a linear combination of M independent source signals $\\mathbf{S} = [\\mathbf{s}_1, \\mathbf{s}_2,\\  \\ldots\\  \\mathbf{s}_\\mathit{m}]^T \\in R^{\\mathit{m}\\times 1}$.The relationship between the ICs and the original variables is given by:\n",
    "\n",
    "28. \n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{As}\n",
    "$$\n",
    " <b>A</b> $\\in \\mathbb{R}^{m\\times m}$ is the unknown mixing matrix.\n",
    " The objective of FastICA can be defined as finding a demixing matrix $\\mathbf{W}\\in R^{m\\times m}$ such that the elements of the reconstructed source vector. \n",
    "\n",
    "29. \n",
    "$$\n",
    " \\mathbf{\\hat{s}} = \\mathbf{Wx}\n",
    "$$\n",
    "are as independent as possible. The eigen-decomposition of the covariance matrix $\\textbf{C}_x=E(\\textbf{xx}^T)\\in R^{M\\times M}$ can be given by\n",
    "\n",
    "29. \n",
    "$$\n",
    "     \\mathbf{C}_x=\\mathbf{V}\\boldsymbol{\\Lambda} \\mathbf{V}^T\n",
    "$$\n",
    "where $\\boldsymbol{\\Lambda} \\in R^{m\\times m}$ is a diagonal matrix of the eigenvalues of $\\mathbf{C}_x$ and $\\mathbf{V}$.\n",
    "The whitening transformation is implemented as:\n",
    "\n",
    "30. \n",
    "$$\n",
    "     \\mathbf{z}=\\mathbf{Qx}=\\boldsymbol{QAs} = \\mathbf{Us}\n",
    "$$\n",
    "\n",
    "<b>z</b> holds the whitened variables, $\\mathbf{Q} = \\mathbf{\\Lambda} ^{-1/2} \\mathbf{V}^T$ is the whitening matrix, $\\mathbf{U} = \\mathbf{QA} = [\\mathbf{u}_1, \\mathbf{u}_2,\\  \\ldots\\  \\mathbf{u}_\\mathit{m}]$ is the orthogonal extracting matrix.\n",
    "The relationship between $\\mathbf{W}$ and $\\mathbf{Q}$ is:\n",
    "\n",
    "31. \n",
    "$$\n",
    "    \\mathbf{W}=\\mathbf{U}^T\\boldsymbol{Q}\n",
    "$$\n",
    "\n",
    "to calculate $\\textbf{U}$ FastICA takes the following negentropy approximation as its objective function.\n",
    "\n",
    "32. \n",
    "$$\n",
    "      \\max_\\mathbf{u}\\ \\ \\ \\{E[G(\\mathbf{u}^T\\mathbf{z})^2]-E[G(v)]\\}^2 \n",
    "$$\n",
    "33. \n",
    "$$\n",
    "s.t.\\ \\ \\ E[(\\mathbf{u}^T\\mathbf{z})^2]=1\n",
    "$$\n",
    "\n",
    "$\\mathbf{u}$ is a column of $\\mathbf{U}$, $v$ is the Gaussian variable with zero mean and unit variance, $G(x)$ is a non quadratic function which is chosen to be $G(x)=-\\exp(-x^2 /2)$.\n",
    "Next, the IC  can be computed using equations demixing and whitening/demixing equations shown earlier.\n",
    "For fault detection, the three monitoring statistics can be defined as\n",
    "\n",
    "34. \n",
    "$$\n",
    "     I^2 = (\\mathbf{U}_p^T\\mathbf{z})^T(\\mathbf{U}_p^T\\mathbf{z})\n",
    "$$\n",
    "35. \n",
    "$$\n",
    "    I_e^2 = (\\mathbf{U}_{\\tilde{p}}^T\\mathbf{z})^T(\\mathbf{U}_{\\tilde{p}}^T\\mathbf{z})\n",
    "$$\n",
    "36. \n",
    "$$\n",
    "     SPE = (\\mathbf{x}-\\mathbf{Q}^{-1}\\mathbf{U}_p\\mathbf{U}_p^T\\mathbf{Qx})^T(\\mathbf{x}-\\mathbf{Q}^{-1}\\mathbf{U}_p\\mathbf{U}_p^T\\mathbf{Qx})\n",
    "$$\n",
    "where $\\textbf{U}_p$ is composed of the first $\\textit{p}$ columns of $\\textbf{U}$.\n",
    "\n",
    "<b>More information on these statistics and their upper control limits can be found below in the <i>Distributions and Statistics</i> section.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Variate Analysis (CVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CVA for fault detection aims to maximize the correlation between past and future data in a given set.\n",
    "For a data-set $\\mathbf{X} \\in R^{n \\times m}$ centered about zero, with $\\textit{m}$ variables and $\\textit{n}$ samples, a past and future vector is constructed for each considered time step. Each vector contains the data points q time steps before/after the point being considered. let $\\mathbf{x}_k$ represent a sample(row) taken from $\\textbf{X}$ at timestep $\\textit{k}$. the past and future vectors are stacked lengthwise.\n",
    "\n",
    "37. \n",
    "$$\n",
    "    \\mathbf{p}_k = [\\mathbf{x}_{k}\\ \\  \\mathbf{x}_{k-1}\\ \\  \\mathbf{x}_{k-2}\\ \\ \\ldots\\ \\  \\mathbf{x}_{k-q+1}]^T \\in R^{mq}\n",
    "$$\n",
    "38.             \n",
    "$$\n",
    "    \\mathbf{f}_k = [\\mathbf{x}_{k+1}\\ \\  \\mathbf{x}_{k+2}\\ \\  \\mathbf{x}_{k+3}\\ \\  \\ldots\\ \\  \\mathbf{x}_{k+q}]^T \\in R^{mq}\n",
    "$$\n",
    "where q is the number of timesteps into the past and future the past and future windows extend, m is the number of variables, and k is the timestep being considered starting at q timesteps from the first observation and ending q steps before the last observation to accommodate for the length of the window. This range can be represented as all $k \\in [q, n-2q]$. \n",
    "\n",
    "Both vectors are normalized to zero mean and unit variance.\n",
    "Then the $\\mathbf{p}_k$ and $\\mathbf{f}_k$ vectors for all $k \\in [q, n-q]$ are appended column-wise to form past and future Hankel matrices.\n",
    "\n",
    "39. \n",
    "$$\n",
    "    \\mathbf{Y}_p=[\\mathbf{p}_{q+1}\\ \\ \\mathbf{p}_{q+2}\\ \\ \\ldots \\ \\ \\mathbf{p}_{n-q}]\\in R^{mq\\times N-2q+1}\n",
    "$$\n",
    "40. \n",
    "$$\n",
    "    \\mathbf{Y}_f=[\\mathbf{f}_{q+1}\\ \\ \\mathbf{f}_{q+2}\\ \\ \\ldots \\ \\ \\mathbf{f}_{n-q}]\\in R^{mq\\times N-2q+1}\n",
    "$$\n",
    "\n",
    "The covariance and cross-covariance of the past and future matrices can be estimated as:\n",
    "\n",
    "41. \n",
    "$$\n",
    "    \\boldsymbol{\\Sigma}_{pp}=\\frac{1}{n-2q}\\mathbf{Y}_p\\mathbf{Y}_p^T \\in R^{mq\\times mq}\n",
    "$$\n",
    "42. \n",
    "$$\n",
    "    \\boldsymbol{\\Sigma}_{ff}=\\frac{1}{n-2q}\\mathbf{Y}_f\\mathbf{Y}_f^T \\in R^{mq\\times mq}\n",
    "$$\n",
    "43. \n",
    "$$\n",
    "    \\boldsymbol{\\Sigma}_{fp}=\\frac{1}{n-2q}\\mathbf{Y}_f\\mathbf{Y}_p^T \\in R^{mq\\times mq}\n",
    "$$\n",
    "\n",
    "Next, to find the linear combination with the maximum correlation, Singular Value Decomposition(SVD) is performed on the Hankel matrix $\\textbf{H}$.\n",
    "\n",
    "44. \n",
    "$$\n",
    "    \\mathbf{H}=\\boldsymbol{\\Sigma}_{ff}^{-1/2}\\boldsymbol{\\Sigma}_{fp}\\boldsymbol{\\Sigma}_{pp}^{-1/2}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^T\n",
    "$$\n",
    "\n",
    "$\\textbf{U}$ and $\\textbf{V}$ are the left and right singular column vectors of $\\textbf{H}$ and $\\boldsymbol{\\Sigma}=diag(\\sigma_1, \\sigma_1, \\sigma_1 \\ldots \\sigma_1, 0 \\ldots 0)$ of singular values ordered from largest to smallest, where $\\textit{r}$ is the rank of $\\textbf{H}$\n",
    "\n",
    "\n",
    "To get the $T^2$ and SPE first the state vector, $z_k$, and the residual vector, $e_k$, must be calculated for each timestep:\n",
    "\n",
    "45. \n",
    "$$\n",
    "    \\mathbf{z}_k=\\mathbf{J}_l\\mathbf{p}_k \\in R^{l}\n",
    "$$\n",
    "50. \n",
    "$$\n",
    "    \\mathbf{e}_k=\\mathbf{F}\\mathbf{p}_k \\in R^{l}\n",
    "$$\n",
    "$\\mathbf{J}_l=\\mathbf{V}_l^T\\boldsymbol{\\Sigma}_{pp}^{-1/2}$, $\\mathbf{F}=(\\mathbf{I}-\\mathbf{V}_l\\mathbf{V}_l^T)\\boldsymbol{\\Sigma}_{pp}^{-1/2}$ and $\\mathbf{V}_l$ is a reduced matrix consisting of the first $\\textit{l}$ columns of $\\textbf{V}$. Matrices $\\mathbf{J}_l$ and $\\mathbf{F}$ represent the projection matrices applied to the past data vectors.\n",
    "\n",
    "Next, $T^2$ and $\\textit{Q}$ statistics are calculated.\n",
    "\n",
    "51. \n",
    "$$\n",
    "    T^2_k = \\mathbf{z}_k^T\\mathbf{z}_k\n",
    "$$\n",
    "52. \n",
    "$$\n",
    "    Q_k = \\mathbf{e}_k^T\\mathbf{e}_k\n",
    "$$\n",
    "\n",
    "<b>More information on these statistics and their upper control limits can be found below in the <i>Distributions and Statistics</i> section.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative clustering aims to produce a hierarchy of agglomerates that can be partitioned down to arbitrarily fine granularity. This is achieved by recursively grouping the pair of data points or clusters that are closest to one another according to a predetermined distance metric. After calculating a pairwise distance matrix(the correlation matrix), the algorithm is as follows:\n",
    "\n",
    "1. Locate the two points or groups that have the smallest distance between them.\n",
    "2. Define the location of the new node, halfway between the two objects.\n",
    "3. Update the distance matrix by removing the two grouped objects and replacing them with the calculated distance of their cluster from each of the other objects.\n",
    "4. Repeat the process until all of the objects are grouped into one agglomerate.\n",
    "\n",
    "Step three requires a method (linkage type) to estimate the distance of the formed cluster to the other objects in the distance matrix. Ward linkage aims to minimize the total within-cluster variance when choosing which objects are grouped. This is accomplished by minimizing the sum squared difference between each point in the newly formed clusters.\n",
    "\n",
    " Clustering was performed on the correlation matrix of the data sets. The correlation matrix is calculated by finding the Pearson correlation coefficient, $r_{xy}$, of each pair of variables ($x$ and $y$) using thi following equation.\n",
    "\n",
    "53. \n",
    "$$\n",
    "    r_{xy} = \\frac{\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2}\\sqrt{\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hotelling's $\\mathbf{T^2}$ Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hotelling's $T^2$ is a multivariate statistical method used to determine the likelihood a sample was drawn from a population.This is done by finding the square Mahalanobis distance $\\mathbf{d}_i$ between their multivariate means as shown in the following equation.\n",
    "\n",
    "54. \n",
    "$$\n",
    "t^2=\\mathbf{d}_i^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{d}_i\n",
    "$$\n",
    "$$\n",
    "t^2=(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\bar{\\mathbf{x}} - \\boldsymbol{\\mu})\n",
    "$$\n",
    "the vectors in the loadings matrix $\\mathbf{P}$ from PCA were drawn along the axes of greatest variance and thus represent the multivariate means of the data. by transorming a data point into the principle space, its coordinates are now equal to the distance from those means. So in the case of PCA it can be said that\n",
    "\n",
    "55. \n",
    "$$\n",
    "\\mathbf{d}_i = (\\bar{\\mathbf{x}}_i - \\boldsymbol{\\mu}) = \\mathbf{z}_i = \\mathbf{P}^T\\mathbf{x}_i\n",
    "$$\n",
    "This means that th Mahalanobis distance would be written as.\n",
    "\n",
    "56. \n",
    "$$\n",
    "t^2=\\mathbf{d}_i^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{d}_i\n",
    "$$\n",
    "$$\n",
    "t^2=\\mathbf{x}_i^T\\mathbf{P}\\boldsymbol{\\Lambda}^{-1}_a\\mathbf{P}^T\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "The Hotelling's $T^2$ disrtibution is very similar to an F-distribution. So if we have a random variable $\\mathbf{t}^2 \\in R^{n\\times a}$ with $a$ variables(principle components in our case) and $n$ samples that follows a $T^2$ distribution, $\\mathbf{t}^2\\sim T^2_{\\mathit{a},\\mathit{n-a}}$, then the data can be transferred to an F-distribution using\n",
    "\n",
    "57. \n",
    "$$\n",
    "\\frac{n-a}{a(n-1)}\\mathbf{t}^2\\sim F_{\\mathit{a},\\mathit{n-a}}\n",
    "$$\n",
    "Thus the the distribution can be written as\n",
    "\n",
    "58. \n",
    "$$\n",
    "\\mathbf{t}^2\\sim T^2_{\\mathit{a},\\mathit{n-a}} = \\frac{a(n-1)}{n-a}F_{\\mathit{a},\\mathit{n-a}}\n",
    "$$\n",
    "The upper contol limit $\\alpha$ for our distribution can be estimated as\n",
    "\n",
    "59. \n",
    "$$\n",
    "T^2_{\\alpha} = \\frac{a(n-1)}{n-a}F_{\\mathit{a},\\mathit{n-a},\\ \\  \\alpha}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotelling's $T^2$\n",
    "$$\n",
    "t^2=(\\bar{\\mathbf{x}}_i - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\bar{\\mathbf{x}}_i - \\boldsymbol{\\mu})\n",
    "$$\n",
    "$$\n",
    "T^2_{\\alpha} = \\frac{a(n-1)}{n-a}F_{\\mathit{a},\\mathit{n-a},\\ \\  \\alpha}\n",
    "$$\n",
    "\n",
    "### SPE\n",
    "$$\n",
    "Q = r_i^Tr_i = \\mathbf{x}_i^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "$$\n",
    "Q_{\\alpha} = \\theta_1 \\left(\\frac{h_0c_{\\alpha}\\sqrt{2\\theta_2}}{\\theta_1} + 1 + \\frac{\\theta_2 h_0 (h_0-1)}{\\theta_1^2}\\right)^{\\frac{1}{h_0}}\n",
    "$$ \n",
    "where \n",
    "$$\n",
    "\\theta_i = \\sum_{j=a+1}^m(\\lambda_j^2)^i\\ , \\ \\ \\ \\ \\ \\  i = 1, 2, 3\\ , \\ \\ \\ \\ \\ \\  h_0=1-\\frac{2\\theta_1 \\theta_3}{3\\theta_2^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Prediction Error (SPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA aims to explain as much of the dataset as possible while reducing the number of dimensions used. This, however, will always result in informationion loss.\n",
    "\n",
    "After performing PCA to get the loadings matrix $\\mathbf{P}$ the data can be transformed into the principle space using the following equation.\n",
    "\n",
    "60. \n",
    "$$\n",
    "\\mathbf{z}_i=\\mathbf{P}^T\\mathbf{x}_i\n",
    "$$\n",
    "where $\\mathbf{x}_i \\in R^{m}$ represents one timestep of the original data,  $\\mathbf{z}_i \\in R^a$ represents the hidden/latent variables for that timestep, and $\\mathbf{P} \\in R^{m \\times a}$ contains the first $a$ columns of the eigenvector matrix.\n",
    "\n",
    "To reconstruct $\\mathbf{x}$ from $\\mathbf{z}$ use\n",
    "\n",
    "61. \n",
    "$$\n",
    "\\mathbf{\\hat{x}}_i = \\mathbf{P}\\mathbf{z}_i = \\mathbf{P}\\mathbf{P}^T\\mathbf{x}_i\n",
    "$$\n",
    "where $\\mathbf{\\hat{x}}_i$ is the reconstructed data. This not identical to $\\mathbf{x}_i$ because information was lost in the PCA transformation.\n",
    "\n",
    "The residuals represent the error of the reconstruction and can be found by taking the difference between $\\mathbf{\\hat{x}}_i$ and $\\mathbf{x}_i$.\n",
    "\n",
    "62. \n",
    "$$\n",
    "r_i = \\mathbf{x}_i - \\mathbf{\\hat{x}}_i = \\mathbf{x}_i - \\mathbf{P}\\mathbf{P}^T\\mathbf{x}_i = (\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "$r_i$ are the residuals(error).\n",
    "to account for negatives we take the square of the residuals. this is expressed as\n",
    "\n",
    "63. \n",
    "$$\n",
    "Q = r_i^Tr_i = \\mathbf{x}_i^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "where Q is the squared prediction error.\n",
    "\n",
    "since $(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)$ is symmetric the above equation can be rewritten as\n",
    "\n",
    "64. \n",
    "$$\n",
    "Q = r_i^Tr_i\n",
    "$$\n",
    "$$\n",
    "Q = \\mathbf{x}_i^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "$$\n",
    "Q = \\mathbf{x}_i^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "$$\n",
    "Q = \\mathbf{x}_i^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T-\\mathbf{P}\\mathbf{P}^T+\\mathbf{P}\\mathbf{P}^T\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "$$\n",
    "Q = \\mathbf{x}_i^T(\\mathbf{I}-\\mathbf{P}\\mathbf{P}^T)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "The upper control limit can be computed from its approximate distribution:\n",
    "\n",
    "65. \n",
    "$$\n",
    "Q_{\\alpha} = \\theta_1 \\left(\\frac{h_0c_{\\alpha}\\sqrt{2\\theta_2}}{\\theta_1} + 1 + \\frac{\\theta_2 h_0 (h_0-1)}{\\theta_1^2}\\right)^{\\frac{1}{h_0}}\n",
    "$$ \n",
    "where\n",
    "\n",
    "66. \n",
    "$$\n",
    "\\theta_i = \\sum_{j=a+1}^m(\\lambda_j^2)^i\\ , \\ \\ \\ \\ \\ \\  i = 1, 2, 3\\ , \\ \\ \\ \\ \\ \\  h_0=1-\\frac{2\\theta_1 \\theta_3}{3\\theta_2^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance statistic, $\\delta_{\\mathbf{S}^2}$, calculates the square difference between the \\\n",
    "covariance of the transformed variables of the training and the testing set.\n",
    "\n",
    "$\\delta_{S^2} = \\left(\\frac{1}{q-1}\\mathbf{Y}^T\\mathbf{Y}- \\frac{1}{n-1}\\mathbf{X}^T\\mathbf{X}\\right)^2$\\\n",
    "\n",
    "$\\textbf{Y} \\in R^{q\\times a}$ represents the PCA-transformed data from $\\textit{q}$ timesteps and $\\textbf{X} \\in R^{n\\times a}$ \\\n",
    "represents the PCA-transformed training data. This is done on both the principle \\\n",
    "and residual space separately. If a fault generates a notable increase in noise, it will \\\n",
    "register even if any individual point would not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation (KDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper control limits (UCL) serve as thresholds between faulty and normal data. In general, UCLs for fault detection are determined analytically. This method relies on the assumption that the underlying process produces data that follows a Gaussian distribution. These solutions are not valid for nonlinear processes under varying conditions. Kernel Density Estimation (KDE) is used to estimate the distribution to accommodate for varying processes as it can provide a more adaptive fit to the data.\n",
    "The kernel used for this paper is the Gaussian kernel.\n",
    "\n",
    "68. \n",
    "$$\n",
    "    K(g) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{g^2}{2}\\right)\n",
    "$$\n",
    "Finding the UCL such that $P(x<UCL)=\\alpha$ is equivalent to finding the upper limit to the integral representing area under the distribution such that the area is equal to $\\alpha$.\n",
    "\n",
    "69. \n",
    "$$\n",
    "    P(x<UCL)=\\int_{-\\infty}^{UCL}\\frac{1}{Mh}\\sum_{k=1}^MK\\left(\\frac{x-x_k}{h}\\right)dx=\\alpha\n",
    "$$\n",
    "$x_k,\\,k=1,2,3\\ldots, M$ are the samples of $x$, and $h$ is the kernel bandwidth.\n",
    "\n",
    "For online monitoring, the statistics are continuously computed from real-time data, and a fault is registered whenever one of the statistics has a value higher than the UCL."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
